{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Youssefkammoun595/RAG_ACADEMIC_PROJECT/blob/main/RAG_DOC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation des dépendances avec améliorations\n",
        "!pip install -U pip # Update pip to the latest version\n",
        "!pip install -q --upgrade bitsandbytes\n",
        "!pip install -q gradio pypdf2 sentence-transformers faiss-cpu transformers torch accelerate langchain-text-splitters rank-bm25\n",
        "!pip install -U bitsandbytes\n",
        "!pip install -q spacy nltk PyPDF2 # Install these packages first\n",
        "!pip install -q pdfplumber # Install pdfplumber separately\n",
        "!pip install langdetect\n",
        "# !pip install -q python-docx textract # Commented out due to potential conflicts/errors\n",
        "!pip install -q setuptools # Ensure setuptools is up-to-date\n",
        "\n",
        "import PyPDF2\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "import faiss\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from typing import List, Tuple, Dict, Any\n",
        "import torch\n",
        "import re\n",
        "import spacy\n",
        "import nltk\n",
        "from collections import defaultdict, Counter\n",
        "from rank_bm25 import BM25Okapi\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\n",
        "import pdfplumber\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Téléchargement des ressources NLTK avec corrections\n",
        "print(\"Downloading NLTK resources...\")\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)  # Ajout de la ressource manquante / Added missing resource\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Configuration avancée avec améliorations / Advanced configuration with improvements\n",
        "CHUNK_SIZE = 1000  # Augmenté pour plus de contexte / Increased for more context\n",
        "CHUNK_OVERLAP = 250  # Augmenté pour mieux préserver le contexte / Increased to better preserve context\n",
        "SENTENCE_CHUNK_SIZE = 384  # Modifié pour correspondre à la limite du modèle (384) / Modified to match model limit\n"
      ],
      "metadata": {
        "id": "521SF9WUAFe-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9df213d-35fb-4d1c-fc5f-56e5a96719d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.3)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.49.0)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.9.0+cpu)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.12/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from langdetect) (1.17.0)\n",
            "Downloading NLTK resources...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedAdvancedRAGSystem:\n",
        "    def __init__(self):\n",
        "        print(\" Loading enhanced models...\")\n",
        "        # Chargement du modèle spaCy pour le NLP / Loading spaCy model for NLP\n",
        "        print(\"    Loading NLP model: fr_core_news_sm...\")\n",
        "        try:\n",
        "            self.nlp = spacy.load(\"fr_core_news_sm\")\n",
        "        except:\n",
        "            print(\"    Installing spaCy model...\")\n",
        "            !python -m spacy download fr_core_news_sm -q\n",
        "            self.nlp = spacy.load(\"fr_core_news_sm\")\n",
        "\n",
        "        # Enhanced embedding model with multilingual support\n",
        "        print(\"    Embedding model: paraphrase-multilingual-MiniLM-L12-v2...\")\n",
        "        self.embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "        # Second embedding model for diversity\n",
        "        print(\"    Second model: all-MiniLM-L6-v2 for diversity...\")\n",
        "        self.embedding_model2 = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "        # Enhanced Cross-Encoder for re-ranking\n",
        "        print(\"    Cross-Encoder for enhanced re-ranking...\")\n",
        "        self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2')\n",
        "\n",
        "        # Modèle de génération optimisé / Optimized generation model\n",
        "        print(\"    Loading LLM: Mistral-7B-Instruct (optimized)...\")\n",
        "        model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.llm = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\",\n",
        "            load_in_8bit=True\n",
        "        )\n",
        "\n",
        "        # Pipeline de génération amélioré / Enhanced generation pipeline\n",
        "        self.generator = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=self.llm,\n",
        "            tokenizer=self.tokenizer,\n",
        "            max_new_tokens=768,  # Augmenté / Increased\n",
        "            do_sample=True,\n",
        "            temperature=0.2,  # Réduit pour plus de précision / Reduced for more precision\n",
        "            top_p=0.9,\n",
        "            top_k=50,\n",
        "            repetition_penalty=1.1,\n",
        "            num_return_sequences=1\n",
        "        )\n",
        "        #  Multiple splitters for different strategies\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=CHUNK_SIZE,\n",
        "            chunk_overlap=CHUNK_OVERLAP,\n",
        "            length_function=len,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \"; \", \", \", \" \", \"\"]\n",
        "        )\n",
        "\n",
        "        self.sentence_splitter = SentenceTransformersTokenTextSplitter(\n",
        "            chunk_overlap=50,\n",
        "            tokens_per_chunk=SENTENCE_CHUNK_SIZE\n",
        "        )\n",
        "\n",
        "        # Structures de données améliorées / Enhanced data structures\n",
        "        self.chunks = []\n",
        "        self.chunk_metadata = []\n",
        "        self.sentence_chunks = []\n",
        "        self.indexes = {}  # Multiple indexes\n",
        "        self.bm25 = None\n",
        "        self.document_analysis = {}\n",
        "        self.keyword_index = defaultdict(list)\n",
        "\n",
        "\n",
        "        print(\" All models loaded with enhancements\")\n",
        "\n",
        "    def extract_text_from_pdf(self, pdf_file) -> Tuple[str, Dict[str, Any]]:\n",
        "        \"\"\" Extracts text with enhanced metadata and structural analysis\"\"\"\n",
        "        try:\n",
        "            text = \"\"\n",
        "            metadata = {\n",
        "                'num_pages': 0,\n",
        "                'page_texts': [],\n",
        "                'tables': [],\n",
        "                'sections': [],\n",
        "                'font_sizes': defaultdict(int)\n",
        "            }\n",
        "\n",
        "            # Essai avec PyPDF2 / Try with PyPDF2\n",
        "            pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "            metadata['num_pages'] = len(pdf_reader.pages)\n",
        "            # Essai avec pdfplumber pour une extraction plus riche / Try with pdfplumber for richer extraction\n",
        "            try:\n",
        "                with pdfplumber.open(pdf_file) as pdf:\n",
        "                    for i, page in enumerate(pdf.pages):\n",
        "                        # Extraction du texte / Text extraction\n",
        "                        page_text = page.extract_text() or \"\"\n",
        "                        if not page_text:\n",
        "                            page_text = pdf_reader.pages[i].extract_text() or \"\"\n",
        "\n",
        "                        #  Table extraction\n",
        "                        tables = page.extract_tables()\n",
        "                        if tables:\n",
        "                            for table in tables:\n",
        "                                metadata['tables'].append({\n",
        "                                    'page': i+1,\n",
        "                                    'table': table\n",
        "                                })\n",
        "\n",
        "                        # Structural analysis\n",
        "                        words = page.extract_words()\n",
        "                        for word in words:\n",
        "                            if 'fontname' in word:\n",
        "                                metadata['font_sizes'][word['fontname']] += 1\n",
        "\n",
        "                        text += f\"\\n--- Page {i+1} ---\\n{page_text}\\n\"\n",
        "                        metadata['page_texts'].append({\n",
        "                            'page_num': i+1,\n",
        "                            'text': page_text,\n",
        "                            'word_count': len(page_text.split()),\n",
        "                            'char_count': len(page_text)\n",
        "                        })\n",
        "            except Exception as e:\n",
        "                print(f\" Erreur pdfplumber, fallback à PyPDF2: {str(e)}\")\n",
        "                print(f\" pdfplumber error, fallback to PyPDF2: {str(e)}\")\n",
        "                # Fallback à PyPDF2 / Fallback to PyPDF2\n",
        "                for i, page in enumerate(pdf_reader.pages):\n",
        "                    page_text = page.extract_text() or \"\"\n",
        "                    text += f\"\\n--- Page {i+1} ---\\n{page_text}\\n\"\n",
        "                    metadata['page_texts'].append({\n",
        "                        'page_num': i+1,\n",
        "                        'text': page_text,\n",
        "                        'word_count': len(page_text.split()),\n",
        "                        'char_count': len(page_text)\n",
        "                    })\n",
        "\n",
        "            #  Automatic section detection\n",
        "            sections = self._detect_sections(text)\n",
        "            metadata['sections'] = sections\n",
        "\n",
        "            return text, metadata\n",
        "        except Exception as e:\n",
        "            return f\"Erreur lors de l'extraction: {str(e)}\", {}\n",
        "\n",
        "    def _detect_sections(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\" Automatically detects document sections\"\"\"\n",
        "        sections = []\n",
        "        lines = text.split('\\n')\n",
        "        current_section = None\n",
        "\n",
        "        for line in lines:\n",
        "            line_stripped = line.strip()\n",
        "            # Détection des titres / Title detection\n",
        "            if (len(line_stripped) < 100 and\n",
        "                (line_stripped.isupper() or\n",
        "                 re.match(r'^(Chapitre|Section|Partie|Titre)\\s+\\d+', line_stripped, re.IGNORECASE) or\n",
        "                 re.match(r'^\\d+[\\.\\s]+\\w+', line_stripped))):\n",
        "\n",
        "                if current_section:\n",
        "                    sections.append(current_section)\n",
        "\n",
        "                current_section = {\n",
        "                    'title': line_stripped,\n",
        "                    'content': [],\n",
        "                    'start_line': len(sections) + 1\n",
        "                }\n",
        "            elif current_section:\n",
        "                current_section['content'].append(line)\n",
        "\n",
        "        if current_section:\n",
        "            sections.append(current_section)\n",
        "\n",
        "        return sections\n",
        "\n",
        "    def analyze_document_structure(self, text: str, metadata: Dict) -> Dict[str, Any]:\n",
        "        \"\"\"Analyse approfondie de la structure du document\"\"\"\n",
        "        doc = self.nlp(text[:9000])  # Analyze first 10k characters\n",
        "\n",
        "        #  Entity extraction\n",
        "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "        # Analyse des parties du discours / Part-of-speech analysis\n",
        "        pos_counts = Counter([token.pos_ for token in doc])\n",
        "\n",
        "        # Extraction des mots-clés\n",
        "        words = [token.text.lower() for token in doc if not token.is_stop and token.is_alpha]\n",
        "        word_freq = Counter(words)\n",
        "        keywords = word_freq.most_common(20)\n",
        "\n",
        "        # SIMPLIFIED LANGUAGE DETECTION - Removed external dependency\n",
        "        # Use a basic rule: if the spaCy model is 'fr_core_news_sm', assume French.\n",
        "        # This is a simple heuristic and can be enhanced.\n",
        "        language = \"fr\"  # Default assumption based on your spaCy model\n",
        "\n",
        "        analysis = {\n",
        "            'entities': entities[:50],  # Limiter à 50 entités / Limit to 50 entities\n",
        "            'pos_distribution': dict(pos_counts),\n",
        "            'top_keywords': keywords,\n",
        "            'language': language,\n",
        "            'reading_level': self._estimate_reading_level(text),\n",
        "            'sentiment': self._analyze_sentiment(text[:5000])\n",
        "        }\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    def _estimate_reading_level(self, text: str) -> str:\n",
        "        \"\"\"Estime le niveau de lecture du texte / Estimates text reading level\"\"\"\n",
        "        try:\n",
        "            words = word_tokenize(text)\n",
        "            sentences = sent_tokenize(text)\n",
        "\n",
        "            if len(words) == 0 or len(sentences) == 0:\n",
        "                return \" Unknown\"\n",
        "\n",
        "            avg_sentence_length = len(words) / len(sentences)\n",
        "            avg_word_length = sum(len(word) for word in words) / len(words)\n",
        "\n",
        "            if avg_sentence_length > 25 and avg_word_length > 5:\n",
        "                return \" Advanced\"\n",
        "            elif avg_sentence_length > 15 and avg_word_length > 4.5:\n",
        "                return \" Intermediate\"\n",
        "            else:\n",
        "                return \" Basic\"\n",
        "        except Exception as e:\n",
        "            print(f\" Error estimating reading level: {str(e)}\")\n",
        "            return \"Unknown\"\n",
        "\n",
        "    def _analyze_sentiment(self, text: str) -> Dict[str, float]:\n",
        "        \"\"\" Simple sentiment analysis\"\"\"\n",
        "        positive_words = {'bon', 'excellent', 'positif', 'bien', 'succès', 'réussi',\n",
        "                         'good', 'excellent', 'positive', 'well', 'success', 'successful'}\n",
        "        negative_words = {'mauvais', 'négatif', 'problème', 'échec', 'difficile',\n",
        "                         'bad', 'negative', 'problem', 'failure', 'difficult'}\n",
        "\n",
        "        words = text.lower().split()\n",
        "        pos_count = sum(1 for word in words if word in positive_words)\n",
        "        neg_count = sum(1 for word in words if word in negative_words)\n",
        "        total = len(words)\n",
        "\n",
        "        if total == 0:\n",
        "            return {'positive': 0, 'negative': 0, 'neutral': 1}\n",
        "\n",
        "        return {\n",
        "            'positive': pos_count / total,\n",
        "            'negative': neg_count / total,\n",
        "            'neutral': 1 - (pos_count + neg_count) / total\n",
        "        }\n",
        "\n",
        "    def preprocess_text(self, text: str) -> str:\n",
        "        \"\"\" Cleans and normalizes text with improvements\"\"\"\n",
        "        # Suppression des caractères spéciaux non désirés / Remove unwanted special characters\n",
        "        text = re.sub(r'[^\\w\\s.,;:!?()-]', ' ', text)  # CORRECTED REGEX\n",
        "        # Normalisation des espaces / Space normalization\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = re.sub(r'\\n+', '\\n', text)\n",
        "        # Correction des espaces autour de la ponctuation / Fix spaces around punctuation\n",
        "        text = re.sub(r'\\s+([.,;:!?)])', r'\\1', text)\n",
        "        text = re.sub(r'([(])\\s+', r'\\1', text)\n",
        "        # Normalisation des guillemets / Quote normalization\n",
        "        text = re.sub(r'[\"\\']', '\"', text)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def chunk_text_advanced(self, text: str, metadata: dict) -> List[dict]:\n",
        "        \"\"\" Chunks text with multiple strategies\"\"\"\n",
        "        text = self.preprocess_text(text)\n",
        "\n",
        "        # Stratégie 1: Chunking par paragraphe/semantique / Strategy 1: Paragraph/semantic chunking\n",
        "        text_chunks = self.text_splitter.split_text(text)\n",
        "\n",
        "        # Stratégie 2: Chunking par phrases pour certains cas / Strategy 2: Sentence chunking for some cases\n",
        "        sentence_chunks = self.sentence_splitter.split_text(text)\n",
        "        self.sentence_chunks = sentence_chunks\n",
        "\n",
        "        enriched_chunks = []\n",
        "        chunk_id = 0\n",
        "\n",
        "        # Enrichissement des chunks principaux / Enrichment of main chunks\n",
        "        for i, chunk in enumerate(text_chunks):\n",
        "            page_num = self._detect_page_number(chunk)\n",
        "\n",
        "            # Analyse du chunk / Chunk analysis\n",
        "            chunk_doc = self.nlp(chunk[:1000])\n",
        "            entities = [(ent.text, ent.label_) for ent in chunk_doc.ents]\n",
        "            keywords = [token.text.lower() for token in chunk_doc\n",
        "                       if not token.is_stop and token.is_alpha][:10]\n",
        "\n",
        "            enriched_chunks.append({\n",
        "                'chunk_id': chunk_id,\n",
        "                'text': chunk,\n",
        "                'page_num': page_num,\n",
        "                'length': len(chunk),\n",
        "                'word_count': len(chunk.split()),\n",
        "                'entities': entities,\n",
        "                'keywords': keywords,\n",
        "                'chunk_type': 'semantic'\n",
        "            })\n",
        "            chunk_id += 1\n",
        "\n",
        "        # Ajout des chunks de phrases pour la diversité / Add sentence chunks for diversity\n",
        "        for i, chunk in enumerate(sentence_chunks):\n",
        "            if len(chunk.split()) > 10:  # Ignorer les chunks trop courts / Ignore too short chunks\n",
        "                enriched_chunks.append({\n",
        "                    'chunk_id': chunk_id,\n",
        "                    'text': chunk,\n",
        "                    'page_num': 0,\n",
        "                    'length': len(chunk),\n",
        "                    'word_count': len(chunk.split()),\n",
        "                    'entities': [],\n",
        "                    'keywords': [],\n",
        "                    'chunk_type': 'sentence'\n",
        "                })\n",
        "                chunk_id += 1\n",
        "\n",
        "        # Construction de l'index de mots-clés / Building keyword index\n",
        "        for chunk in enriched_chunks:\n",
        "            for keyword in chunk['keywords']:\n",
        "                self.keyword_index[keyword].append(chunk['chunk_id'])\n",
        "\n",
        "        return enriched_chunks\n",
        "\n",
        "    def _detect_page_number(self, chunk: str) -> int:\n",
        "        \"\"\" Detects page number in chunk\"\"\"\n",
        "        match = re.search(r'--- Page (\\d+) ---', chunk)\n",
        "        if match:\n",
        "            return int(match.group(1))\n",
        "        return 0\n",
        "\n",
        "    def create_enhanced_embeddings(self, chunks: List[dict]) -> Dict[str, np.ndarray]:\n",
        "        \"\"\" Creates multiple embeddings for better representation\"\"\"\n",
        "        texts = [c['text'] for c in chunks]\n",
        "\n",
        "        # Embeddings du modèle principal / Main model embeddings\n",
        "        embeddings1 = self.embedding_model.encode(\n",
        "            texts,\n",
        "            show_progress_bar=True,\n",
        "            batch_size=16,\n",
        "            normalize_embeddings=True,\n",
        "            convert_to_numpy=True\n",
        "        )\n",
        "\n",
        "        # Embeddings du deuxième modèle / Second model embeddings\n",
        "        embeddings2 = self.embedding_model2.encode(\n",
        "            texts,\n",
        "            show_progress_bar=False,\n",
        "            batch_size=16,\n",
        "            normalize_embeddings=True,\n",
        "            convert_to_numpy=True\n",
        "        )\n",
        "\n",
        "        # Fusion des embeddings (concatenation) / Embedding fusion (concatenation)\n",
        "        if embeddings1.shape[0] == embeddings2.shape[0]:\n",
        "            combined_embeddings = np.concatenate([embeddings1, embeddings2], axis=1)\n",
        "        else:\n",
        "            combined_embeddings = embeddings1\n",
        "\n",
        "        return {\n",
        "            'primary': embeddings1,\n",
        "            'secondary': embeddings2,\n",
        "            'combined': combined_embeddings\n",
        "        }\n",
        "    def build_advanced_indexes(self, embeddings: Dict[str, np.ndarray], chunks: List[dict]):\n",
        "        \"\"\"Construit des indexes multiples et spécialisés / Builds multiple specialized indexes\"\"\"\n",
        "\n",
        "        # Index FAISS pour embeddings combinés / FAISS index for combined embeddings\n",
        "        dimension = embeddings['combined'].shape[1]\n",
        "        self.indexes['combined'] = faiss.IndexFlatIP(dimension)\n",
        "        self.indexes['combined'].add(embeddings['combined'].astype('float32'))\n",
        "\n",
        "        # Index FAISS pour embeddings primaires / FAISS index for primary embeddings\n",
        "        dimension1 = embeddings['primary'].shape[1]\n",
        "        self.indexes['primary'] = faiss.IndexFlatIP(dimension1)\n",
        "        self.indexes['primary'].add(embeddings['primary'].astype('float32'))\n",
        "\n",
        "        # Index BM25\n",
        "        tokenized_chunks = [c['text'].lower().split() for c in chunks]\n",
        "        self.bm25 = BM25Okapi(tokenized_chunks)\n",
        "\n",
        "        # Index par entités / Entity index\n",
        "        self._build_entity_index(chunks)\n",
        "        print(f\" Indexes created: Combined FAISS ({dimension}D), Primary FAISS ({dimension1}D), BM25, Entities\")\n",
        "\n",
        "    def _build_entity_index(self, chunks: List[dict]):\n",
        "        \"\"\"Construit un index basé sur les entités nommées / Builds index based on named entities\"\"\"\n",
        "        self.entity_index = defaultdict(list)\n",
        "        for chunk in chunks:\n",
        "            for entity, label in chunk.get('entities', []):\n",
        "                self.entity_index[entity.lower()].append(chunk['chunk_id'])\n",
        "\n",
        "    def process_pdf(self, pdf_file) -> str:\n",
        "        \"\"\"Traite le PDF avec pipeline avancé amélioré / Processes PDF with enhanced advanced pipeline\"\"\"\n",
        "        if pdf_file is None:\n",
        "            return \" Veuillez uploader un fichier PDF / Please upload a PDF file\"\n",
        "\n",
        "        try:\n",
        "            # Réinitialisation des données / Data reset\n",
        "            self.chunks = []\n",
        "            self.chunk_metadata = []\n",
        "            self.indexes = {}\n",
        "            self.keyword_index = defaultdict(list)\n",
        "\n",
        "            text, metadata = self.extract_text_from_pdf(pdf_file)\n",
        "\n",
        "            if text.startswith(\"Erreur\"):\n",
        "                return text\n",
        "\n",
        "            # Analyse approfondie du document / In-depth document analysis\n",
        "            self.document_analysis = self.analyze_document_structure(text, metadata)\n",
        "\n",
        "            # Chunking avancé / Advanced chunking\n",
        "            self.chunk_metadata = self.chunk_text_advanced(text, metadata)\n",
        "            self.chunks = [c['text'] for c in self.chunk_metadata]\n",
        "\n",
        "            if len(self.chunks) == 0:\n",
        "                return \"  No text extracted from PDF\"\n",
        "\n",
        "            # Création d'embeddings améliorés / Creation of enhanced embeddings\n",
        "            embeddings = self.create_enhanced_embeddings(self.chunk_metadata)\n",
        "\n",
        "            # Construction d'indexes avancés / Building advanced indexes\n",
        "            self.build_advanced_indexes(embeddings, self.chunk_metadata)\n",
        "\n",
        "            # Statistiques avancées / Advanced statistics\n",
        "            stats = self._compute_advanced_stats()\n",
        "\n",
        "            # Rapport détaillé / Detailed report\n",
        "            report = self._generate_processing_report(metadata, stats)\n",
        "\n",
        "            return report\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\" Erreur: {str(e)} / Error: {str(e)}\"\n",
        "\n",
        "    def _compute_advanced_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\" Computes advanced statistics on the document\"\"\"\n",
        "        total_words = sum(c['word_count'] for c in self.chunk_metadata)\n",
        "        total_chars = sum(c['length'] for c in self.chunk_metadata)\n",
        "        avg_word_length = total_chars / total_words if total_words > 0 else 0\n",
        "\n",
        "        # Comptage des entités / Entity counting\n",
        "        all_entities = []\n",
        "        for chunk in self.chunk_metadata:\n",
        "            all_entities.extend([e[0] for e in chunk.get('entities', [])])\n",
        "\n",
        "        entity_counts = Counter(all_entities)\n",
        "\n",
        "        return {\n",
        "            'total_chunks': len(self.chunks),\n",
        "            'total_words': total_words,\n",
        "            'total_chars': total_chars,\n",
        "            'avg_word_length': avg_word_length,\n",
        "            'top_entities': entity_counts.most_common(10),\n",
        "            'unique_entities': len(set(all_entities)),\n",
        "            'keyword_density': len(self.keyword_index) / total_words if total_words > 0 else 0\n",
        "        }\n",
        "\n",
        "    def _generate_processing_report(self, metadata: Dict, stats: Dict) -> str:\n",
        "        \"\"\"Génère un rapport de traitement détaillé / Generates detailed processing report\"\"\"\n",
        "\n",
        "        # Analyse du document / Document analysis\n",
        "        doc_analysis = self.document_analysis\n",
        "\n",
        "        report = f\"\"\"\n",
        " **ADVANCED DOCUMENT ANALYSIS**\n",
        "\n",
        "**  General Information:**\n",
        "  • Pages: {metadata['num_pages']}\n",
        "  •  Chunks created: {stats['total_chunks']}\n",
        "  •  Total words: {stats['total_words']}\n",
        "  •  Characters: {stats['total_chars']:,}\n",
        "\n",
        "**  Structural Analysis:**\n",
        "  •  Reading level: {doc_analysis.get('reading_level', ' Unknown')}\n",
        "  •  Language detected: {doc_analysis.get('language', 'fr')}\n",
        "  •  Sections detected: {len(metadata.get('sections', []))}\n",
        "  •  Tables extracted: {len(metadata.get('tables', []))}\n",
        "\n",
        "**  Text Statistics:**\n",
        "  •  Average word length: {stats['avg_word_length']:.2f} caractères\n",
        "  •  Keyword density: {stats['keyword_density']:.3%}\n",
        "  •  Unique entities: {stats['unique_entities']}\n",
        "\n",
        "**  Main Keywords:**\n",
        "\"\"\"\n",
        "        for keyword, count in doc_analysis.get('top_keywords', [])[:10]:\n",
        "            report += f\"  • {keyword}: {count} occurrences\\n\"\n",
        "\n",
        "        sentiment = doc_analysis.get('sentiment', {})\n",
        "        report += f\"\"\"\n",
        "**  General Sentiment:**\n",
        "  •  Positive: {sentiment.get('positive', 0):.1%}\n",
        "  •  Negative: {sentiment.get('negative', 0):.1%}\n",
        "  •  Neutral: {sentiment.get('neutral', 0):.1%}\n",
        "\n",
        "** Modèles utilisés / Models Used:**\n",
        "  • Embeddings: paraphrase-multilingual-MiniLM-L12-v2 + all-MiniLM-L6-v2\n",
        "  • Re-ranking: cross-encoder/ms-marco-MiniLM-L-12-v2\n",
        "  • LLM: Mistral-7B-Instruct (8-bit quantized)\n",
        "  • NLP: spaCy fr_core_news_sm\n",
        "\n",
        "**  Analysis Capabilities:**\n",
        "  ✓ Analyse sémantique avancée / Advanced semantic analysis\n",
        "  ✓ Détection d'entités nommées / Named entity recognition\n",
        "  ✓ Analyse de sentiment / Sentiment analysis\n",
        "  ✓ Extraction de structure / Structure extraction\n",
        "  ✓ Indexation multi-modèles / Multi-model indexing\n",
        "  ✓ Recherche hybride améliorée / Enhanced hybrid search\n",
        "\"\"\"\n",
        "\n",
        "        return report\n",
        "\n",
        "    def analyze_query(self, query: str) -> Dict[str, Any]:\n",
        "        \"\"\" In-depth analysis of user query\"\"\"\n",
        "        doc = self.nlp(query)\n",
        "        # Classification du type de question / Question type classification\n",
        "        question_types = {\n",
        "            'factual': ['quel', 'quelle', 'quand', 'où', 'qui', 'combien',\n",
        "                       'what', 'when', 'where', 'who', 'how many'],\n",
        "            'analytical': ['pourquoi', 'comment', 'analyse', 'explique',\n",
        "                          'why', 'how', 'analyze', 'explain'],\n",
        "            'comparative': ['compare', 'différence', 'similaire', 'contraire',\n",
        "                           'compare', 'difference', 'similar', 'contrary'],\n",
        "            'summarization': ['résume', 'résumé', 'synthèse', 'principaux points',\n",
        "                             'summarize', 'summary', 'synthesis', 'main points'],\n",
        "            'extraction': ['liste', 'extrais', 'données', 'statistiques',\n",
        "                          'list', 'extract', 'data', 'statistics'],\n",
        "            'evaluative': ['évalue', 'critique', 'apprécie', 'juge',\n",
        "                          'evaluate', 'critique', 'appreciate', 'judge']\n",
        "        }\n",
        "\n",
        "        q_type = 'général / general'\n",
        "        for type_key, keywords in question_types.items():\n",
        "            if any(keyword in query.lower() for keyword in keywords):\n",
        "                q_type = type_key\n",
        "                break\n",
        "\n",
        "        #  Entity and keyword extraction\n",
        "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "        keywords = [token.text.lower() for token in doc\n",
        "                   if not token.is_stop and token.is_alpha]\n",
        "\n",
        "        # Complexité de la requête / Query complexity\n",
        "        word_count = len(query.split())\n",
        "        sentence_count = len(sent_tokenize(query))\n",
        "        complexity = 'simple' if word_count < 10 else 'complexe / complex' if word_count > 25 else 'moyenne / medium'\n",
        "\n",
        "        return {\n",
        "            'type': q_type,\n",
        "            'entities': entities,\n",
        "            'keywords': keywords,\n",
        "            'complexity': complexity,\n",
        "            'word_count': word_count,\n",
        "            'sentence_count': sentence_count,\n",
        "            'requires_context': word_count > 15 or q_type in ['analytical', 'comparative', 'evaluative']\n",
        "        }\n",
        "\n",
        "    def enhanced_hybrid_retrieve(self, query: str, query_analysis: Dict, k: int = 20) -> List[int]:\n",
        "        \"\"\" Enhanced hybrid retrieval with query analysis\"\"\"\n",
        "        if not self.indexes or not self.bm25:\n",
        "            return []\n",
        "\n",
        "        # Ajustement dynamique de k basé sur la complexité / Dynamic k adjustment based on complexity\n",
        "        if query_analysis['complexity'] == 'complexe / complex':\n",
        "            k = 25\n",
        "        elif query_analysis['type'] in ['comparative', 'analytical']:\n",
        "            k = 30\n",
        "\n",
        "        #  Multiple semantic search\n",
        "        query_embedding1 = self.embedding_model.encode([query], normalize_embeddings=True)\n",
        "        query_embedding2 = self.embedding_model2.encode([query], normalize_embeddings=True)\n",
        "        # Recherche dans l'index combiné / Search in combined index\n",
        "        semantic_scores1, semantic_indices1 = self.indexes['combined'].search(\n",
        "            np.concatenate([query_embedding1, query_embedding2], axis=1).astype('float32'), k\n",
        "        )\n",
        "        #  Search in primary index\n",
        "        semantic_scores2, semantic_indices2 = self.indexes['primary'].search(\n",
        "            query_embedding1.astype('float32'), k\n",
        "        )\n",
        "        # Recherche lexicale (BM25) / Lexical search (BM25)\n",
        "        tokenized_query = query.lower().split()\n",
        "        bm25_scores = self.bm25.get_scores(tokenized_query)\n",
        "        bm25_indices = np.argsort(bm25_scores)[::-1][:k]\n",
        "\n",
        "        # Recherche par entités / Entity search\n",
        "        entity_indices = []\n",
        "        for entity, _ in query_analysis['entities']:\n",
        "            entity_indices.extend(self.entity_index.get(entity.lower(), []))\n",
        "\n",
        "        # Fusion intelligente avec poids / Intelligent fusion with weights\n",
        "        combined_scores = defaultdict(float)\n",
        "\n",
        "        # Poids basés sur le type de question / Weights based on question type\n",
        "        if query_analysis['type'] == 'factual':\n",
        "            weights = {'semantic': 0.4, 'lexical': 0.5, 'entity': 0.1}\n",
        "        elif query_analysis['type'] == 'analytical':\n",
        "            weights = {'semantic': 0.6, 'lexical': 0.3, 'entity': 0.1}\n",
        "        else:\n",
        "            weights = {'semantic': 0.5, 'lexical': 0.4, 'entity': 0.1}\n",
        "\n",
        "        # Score sémantique (index combiné) / Semantic score (combined index)\n",
        "        for rank, idx in enumerate(semantic_indices1[0]):\n",
        "            combined_scores[idx] += weights['semantic'] * (1 / (rank + 60))\n",
        "\n",
        "        # Score lexical / Lexical score\n",
        "        for rank, idx in enumerate(bm25_indices):\n",
        "            combined_scores[idx] += weights['lexical'] * (1 / (rank + 60))\n",
        "\n",
        "        # Score par entités / Entity score\n",
        "        entity_indices = list(set(entity_indices))[:k]\n",
        "        for rank, idx in enumerate(entity_indices):\n",
        "            combined_scores[idx] += weights['entity'] * (1 / (rank + 60))\n",
        "\n",
        "        # Tri et sélection / Sorting and selection\n",
        "        sorted_indices = sorted(\n",
        "            combined_scores.items(),\n",
        "            key=lambda x: x[1],\n",
        "            reverse=True\n",
        "        )\n",
        "\n",
        "        return [idx for idx, _ in sorted_indices[:k]]\n",
        "\n",
        "    def enhanced_rerank(self, query: str, chunk_indices: List[int], top_k: int = 6) -> List[int]:\n",
        "        \"\"\"Re-ranking amélioré avec diversité / Enhanced re-ranking with diversity\"\"\"\n",
        "        if not chunk_indices:\n",
        "            return []\n",
        "\n",
        "        # Re-ranking avec Cross-Encoder / Re-ranking with Cross-Encoder\n",
        "        pairs = [[query, self.chunks[idx]] for idx in chunk_indices]\n",
        "        scores = self.reranker.predict(pairs)\n",
        "\n",
        "        # Sélection avec diversité / Selection with diversity\n",
        "        selected_indices = []\n",
        "        selected_chunks = []\n",
        "\n",
        "        sorted_pairs = sorted(zip(chunk_indices, scores), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        for idx, score in sorted_pairs:\n",
        "            chunk_text = self.chunks[idx]\n",
        "\n",
        "            # Vérifier la similarité / Check similarity\n",
        "            if len(selected_chunks) > 0:\n",
        "                similarities = [self._compute_text_similarity(chunk_text, selected_chunk)\n",
        "                              for selected_chunk in selected_chunks]\n",
        "                if any(sim > 0.8 for sim in similarities):  # Seuil de similarité / Similarity threshold\n",
        "                    continue\n",
        "\n",
        "            selected_indices.append(idx)\n",
        "            selected_chunks.append(chunk_text[:500])  # Garder une version courte / Keep short version\n",
        "\n",
        "            if len(selected_indices) >= top_k:\n",
        "                break\n",
        "\n",
        "        return selected_indices\n",
        "\n",
        "    def _compute_text_similarity(self, text1: str, text2: str) -> float:\n",
        "        \"\"\"Calcule la similarité entre deux textes \"\"\"\n",
        "        if not text1 or not text2:\n",
        "            return 0.0\n",
        "\n",
        "        # Méthode simple basée sur le vocabulaire commun / Simple method based on common vocabulary\n",
        "        words1 = set(text1.lower().split()[:50])\n",
        "        words2 = set(text2.lower().split()[:50])\n",
        "\n",
        "        if not words1 or not words2:\n",
        "            return 0.0\n",
        "\n",
        "        intersection = len(words1.intersection(words2))\n",
        "        union = len(words1.union(words2))\n",
        "\n",
        "        return intersection / union if union > 0 else 0.0\n",
        "\n",
        "    def construct_intelligent_context(self, query: str, chunk_indices: List[int], query_analysis: Dict) -> Tuple[str, List[Dict]]:\n",
        "        \"\"\"Construit un contexte intelligent basé sur l'analyse de la requête \"\"\"\n",
        "        relevant_chunks = []\n",
        "\n",
        "        for rank, idx in enumerate(chunk_indices):\n",
        "            chunk_data = {\n",
        "                'text': self.chunks[idx],\n",
        "                'metadata': self.chunk_metadata[idx],\n",
        "                'rank': rank + 1,\n",
        "                'relevance_score': 1.0 / (rank + 1)  # Score de pertinence simple / Simple relevance score\n",
        "            }\n",
        "            relevant_chunks.append(chunk_data)\n",
        "\n",
        "        #  Context organization based on question type\n",
        "        context_parts = []\n",
        "\n",
        "        if query_analysis['type'] == 'factual':\n",
        "            #  For factual questions\n",
        "            relevant_chunks.sort(key=lambda x: len(x['metadata'].get('entities', [])), reverse=True)\n",
        "\n",
        "        elif query_analysis['type'] == 'analytical':\n",
        "            #  For analytical questions\n",
        "            relevant_chunks = relevant_chunks[:8]  #  More chunks\n",
        "\n",
        "        elif query_analysis['type'] == 'comparative':\n",
        "            #  For comparative questions\n",
        "            relevant_chunks = self._organize_chunks_by_theme(relevant_chunks, query)\n",
        "\n",
        "        # Construction du contexte formaté / Building formatted context\n",
        "        for chunk in relevant_chunks:\n",
        "            metadata = chunk['metadata']\n",
        "            page_info = f\"[Page {metadata['page_num']}]\" if metadata['page_num'] > 0 else \"\"\n",
        "\n",
        "            # Ajout d'information sur le type de chunk / Adding chunk type information\n",
        "            chunk_type = metadata.get('chunk_type', 'standard')\n",
        "            type_info = f\"({chunk_type})\" if chunk_type != 'semantic' else \"\"\n",
        "\n",
        "            context_parts.append(f\"{page_info} {type_info}\\n{chunk['text']}\")\n",
        "\n",
        "        context = \"\\n\\n---\\n\\n\".join(context_parts)\n",
        "\n",
        "        # Limite dynamique basée sur la complexité / Dynamic limit based on complexity\n",
        "        if query_analysis['complexity'] == 'simple':\n",
        "            max_context = 2000\n",
        "        elif query_analysis['complexity'] == 'complexe / complex':\n",
        "            max_context = 4000\n",
        "        else:\n",
        "            max_context = 3000\n",
        "\n",
        "        return context[:max_context], relevant_chunks\n",
        "\n",
        "    def _organize_chunks_by_theme(self, chunks: List[Dict], query: str) -> List[Dict]:\n",
        "        \"\"\" Organizes chunks by theme for comparative questions\"\"\"\n",
        "        #  Simple organization by keywords\n",
        "        query_words = set(query.lower().split())\n",
        "        chunk_scores = []\n",
        "\n",
        "        for chunk in chunks:\n",
        "            chunk_words = set(chunk['metadata'].get('keywords', []))\n",
        "            common_words = len(query_words.intersection(chunk_words))\n",
        "            chunk_scores.append((chunk, common_words))\n",
        "\n",
        "        #  Sort by number of common words\n",
        "        chunk_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "        return [chunk for chunk, _ in chunk_scores]\n",
        "\n",
        "    def generate_enhanced_prompt(self, context: str, question: str, query_analysis: Dict) -> str:\n",
        "        \"\"\" Generates enhanced prompt based on query analysis\"\"\"\n",
        "\n",
        "        #  Specific instructions by question type\n",
        "        type_instructions = {\n",
        "            'factual': \"\"\"\n",
        " Specific instructions:\n",
        "- Fournis une réponse factuelle et précise / Provide factual and precise answer\n",
        "- Cite les sources exactes (pages, sections) / Cite exact sources (pages, sections)\n",
        "- Donne des chiffres et dates si disponibles / Provide numbers and dates if available\n",
        "- Sois concis et direct / Be concise and direct\"\"\",\n",
        "\n",
        "            'analytical': \"\"\"\n",
        "Instructions spécifiques / Specific instructions:\n",
        "- Analyse en profondeur les causes et conséquences / Analyze causes and consequences in depth\n",
        "- Identifie les modèles et tendances / Identify patterns and trends\n",
        "- Fais des liens entre différentes parties du document / Make connections between different parts of the document\n",
        "- Propose une interprétation raisonnée / Propose reasoned interpretation\"\"\",\n",
        "\n",
        "            'comparative': \"\"\"\n",
        "Instructions spécifiques / Specific instructions:\n",
        "- Compare systématiquement les éléments demandés / Systematically compare requested elements\n",
        "- Identifie les similitudes et différences / Identify similarities and differences\n",
        "- Structure ta réponse en points comparatifs / Structure your answer in comparative points\n",
        "- Donne des exemples concrets pour chaque point / Provide concrete examples for each point\"\"\",\n",
        "\n",
        "            'summarization': \"\"\"\n",
        "Instructions spécifiques / Specific instructions:\n",
        "- Résume l'essentiel sans détails superflus / Summarize essentials without unnecessary details\n",
        "- Structure en points clés (3-5 points maximum) / Structure in key points (3-5 points maximum)\n",
        "- Inclus les conclusions principales / Include main conclusions\n",
        "- Conserve le ton et le style du document original / Preserve original document tone and style\"\"\",\n",
        "\n",
        "            'extraction': \"\"\"\n",
        "Instructions spécifiques / Specific instructions:\n",
        "- Liste les éléments demandés de manière organisée / List requested elements in organized manner\n",
        "- Donne les données exactes telles que présentes / Provide exact data as present\n",
        "- Précise la localisation (page, section) / Specify location (page, section)\n",
        "- Présente sous forme de tableau si approprié / Present in table format if appropriate\"\"\",\n",
        "\n",
        "            'evaluative': \"\"\"\n",
        "Instructions spécifiques / Specific instructions:\n",
        "- Évalue de manière objective et équilibrée / Evaluate objectively and balanced\n",
        "- Présente les points forts et faibles / Present strengths and weaknesses\n",
        "- Appuie ton évaluation sur des preuves du document / Base your evaluation on document evidence\n",
        "- Sois constructif dans tes recommandations / Be constructive in your recommendations\"\"\"\n",
        "        }\n",
        "\n",
        "        instructions = type_instructions.get(query_analysis['type'], \"\"\"\n",
        "Instructions générales / General instructions:\n",
        "- Réponds de manière précise et structurée / Answer precisely and structured\n",
        "- Cite les numéros de page quand pertinent / Cite page numbers when relevant\n",
        "- Si l'information n'est pas dans le contexte, dis-le clairement / If information not in context, say so clearly\n",
        "- Sois complet mais concis / Be complete but concise\"\"\")\n",
        "\n",
        "        prompt = f\"\"\"<s>[INST] Tu es un assistant expert en analyse de documents avec capacités avancées.\n",
        "You are an expert document analysis assistant with advanced capabilities.\n",
        "\n",
        "##  DOCUMENT CONTEXT:\n",
        "{context[:3500]}\n",
        "\n",
        "##  QUESTION TO ANALYZE:\n",
        "{question}\n",
        "\n",
        "## TYPE DE QUESTION / QUESTION TYPE: {query_analysis['type'].upper()}\n",
        "## COMPLEXITÉ / COMPLEXITY: {query_analysis['complexity'].upper()}\n",
        "\n",
        "{instructions}\n",
        "\n",
        "##  EXPECTED RESPONSE FORMAT:\n",
        "1. Commence par une réponse directe à la question / Start with direct answer to question\n",
        "2. Développe avec des arguments structurés / Develop with structured arguments\n",
        "3. Cite tes sources avec précision / Cite your sources precisely\n",
        "4. Termine par une synthèse si pertinent / End with synthesis if relevant\n",
        "\n",
        "Réponds UNIQUEMENT en français et uniquement basé sur le contexte fourni.\n",
        "Respond ONLY in French and ONLY based on provided context. [/INST]\n",
        "\n",
        " Expert response:\"\"\"\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def answer_question(self, question: str) -> str:\n",
        "        \"\"\" Enhanced RAG with advanced analysis\"\"\"\n",
        "        if not self.indexes:\n",
        "            return \"  Please first upload and process a PDF\"\n",
        "\n",
        "        if not question or question.strip() == \"\":\n",
        "            return \" Veuillez poser une question / Please ask a question\"\n",
        "\n",
        "        try:\n",
        "            # 1. Analyse approfondie de la requête / In-depth query analysis\n",
        "            query_analysis = self.analyze_query(question)\n",
        "\n",
        "            print(f\" Analyse de la requête / Query analysis: Type={query_analysis['type']}, Complexité / Complexity={query_analysis['complexity']}\")\n",
        "\n",
        "            # 2. Recherche hybride améliorée / Enhanced hybrid retrieval\n",
        "            candidate_indices = self.enhanced_hybrid_retrieve(question, query_analysis, k=25)\n",
        "\n",
        "            if not candidate_indices:\n",
        "                return \" Aucun contexte pertinent trouvé dans le document / No relevant context found in document\"\n",
        "\n",
        "            print(f\"🔍 {len(candidate_indices)} candidats trouvés / candidates found\")\n",
        "\n",
        "            # 3. Re-ranking amélioré / Enhanced re-ranking\n",
        "            top_indices = self.enhanced_rerank(question, candidate_indices, top_k=8)\n",
        "\n",
        "            if not top_indices:\n",
        "                return \" Aucun chunk pertinent après re-ranking / No relevant chunks after re-ranking\"\n",
        "\n",
        "            print(f\"🎯 {len(top_indices)} chunks sélectionnés après re-ranking / chunks selected after re-ranking\")\n",
        "\n",
        "            # 4. Construction de contexte intelligent / Intelligent context construction\n",
        "            context, relevant_chunks = self.construct_intelligent_context(\n",
        "                question, top_indices, query_analysis\n",
        "            )\n",
        "\n",
        "            # 5. Génération de prompt amélioré / Enhanced prompt generation\n",
        "            prompt = self.generate_enhanced_prompt(context, question, query_analysis)\n",
        "\n",
        "            # 6. Génération avec paramètres adaptatifs / Generation with adaptive parameters\n",
        "            generation_params = {\n",
        "                'max_new_tokens': 1024 if query_analysis['complexity'] == 'complexe / complex' else 768,\n",
        "                'do_sample': True,\n",
        "                'temperature': 0.1 if query_analysis['type'] == 'factual' else 0.2,\n",
        "                'top_p': 0.9,\n",
        "                'top_k': 40,\n",
        "                'repetition_penalty': 1.05,\n",
        "                'num_return_sequences': 1\n",
        "            }\n",
        "\n",
        "            print(\" Génération de la réponse en cours... / Generating response...\")\n",
        "            response = self.generator(prompt, **generation_params)\n",
        "\n",
        "            answer = response[0]['generated_text']\n",
        "\n",
        "            # Extraction de la réponse / Response extraction\n",
        "            if \"Réponse experte / Expert response:\" in answer:\n",
        "                answer = answer.split(\"Réponse experte / Expert response:\")[-1].strip()\n",
        "            elif \"Réponse experte:\" in answer:\n",
        "                answer = answer.split(\"Réponse experte:\")[-1].strip()\n",
        "            elif \"[/INST]\" in answer:\n",
        "                answer = answer.split(\"[/INST]\")[-1].strip()\n",
        "\n",
        "            # 7. Post-processing et formatage / Post-processing and formatting\n",
        "            answer = self.postprocess_answer(answer)\n",
        "\n",
        "            # 8. Génération du rapport détaillé / Detailed report generation\n",
        "            report = self.generate_detailed_report(\n",
        "                answer, relevant_chunks, query_analysis, context\n",
        "            )\n",
        "\n",
        "            return report\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\" Erreur lors de l'analyse / Error during analysis: {str(e)}\"\n",
        "\n",
        "    def postprocess_answer(self, answer: str) -> str:\n",
        "        \"\"\"Post-traitement de la réponse pour améliorer la qualité / Post-processing answer to improve quality\"\"\"\n",
        "        try:\n",
        "            # Suppression des répétitions / Remove repetitions\n",
        "            sentences = sent_tokenize(answer)\n",
        "            unique_sentences = []\n",
        "            seen_sentences = set()\n",
        "\n",
        "            for sentence in sentences:\n",
        "                sentence_clean = sentence.strip()\n",
        "                if sentence_clean and sentence_clean not in seen_sentences:\n",
        "                    seen_sentences.add(sentence_clean)\n",
        "                    unique_sentences.append(sentence_clean)\n",
        "\n",
        "            # Reformattage / Reformatting\n",
        "            processed_answer = ' '.join(unique_sentences)\n",
        "\n",
        "            # Amélioration de la structure / Structure improvement\n",
        "            processed_answer = re.sub(r'\\s+([.,;:!?)])', r'\\1', processed_answer)\n",
        "            processed_answer = re.sub(r'([(])\\s+', r'\\1', processed_answer)\n",
        "            processed_answer = processed_answer.replace(' .', '.').replace(' ,', ',')\n",
        "\n",
        "            return processed_answer.strip()\n",
        "        except Exception as e:\n",
        "            print(f\" Erreur post-traitement: {str(e)}\")\n",
        "            print(f\" Post-processing error: {str(e)}\")\n",
        "            return answer\n",
        "\n",
        "    def generate_detailed_report(self, answer: str, relevant_chunks: List[Dict],\n",
        "                                query_analysis: Dict, context: str) -> str:\n",
        "        \"\"\"Génère un rapport détaillé de l'analyse / Generates detailed analysis report\"\"\"\n",
        "\n",
        "        # Sources utilisées / Sources used\n",
        "        sources_info = \"\\n\".join([\n",
        "            f\"  • Chunk {c['rank']}: Page {c['metadata']['page_num']} \"\n",
        "            f\"({c['metadata']['word_count']} mots / words, {c['metadata'].get('chunk_type', 'standard')})\"\n",
        "            for c in relevant_chunks[:6]\n",
        "        ])\n",
        "\n",
        "        # Statistiques des chunks / Chunk statistics\n",
        "        chunk_stats = {\n",
        "            'total_chunks': len(relevant_chunks),\n",
        "            'avg_word_count': np.mean([c['metadata']['word_count'] for c in relevant_chunks]),\n",
        "            'total_pages': len(set(c['metadata']['page_num'] for c in relevant_chunks)),\n",
        "            'entities_found': sum(len(c['metadata'].get('entities', [])) for c in relevant_chunks)\n",
        "        }\n",
        "\n",
        "        # Contexte extrait / Extracted context\n",
        "        context_preview = context[:1200] + \"...\" if len(context) > 1200 else context\n",
        "\n",
        "        report = f\"\"\"\n",
        " * ADVANCED RESPONSE (Mistral-7B + Enhanced RAG)\n",
        "    - RESPONSE:\n",
        "{answer}\n",
        "\n",
        "---\n",
        "\n",
        "###  **ANALYSE DE LA REQUÊTE / QUERY ANALYSIS:**\n",
        "  • **Type de question / Question type:** {query_analysis['type'].upper()}\n",
        "  • **Complexité / Complexity:** {query_analysis['complexity'].upper()}\n",
        "  • **Mots-clés détectés / Keywords detected:** {', '.join(query_analysis['keywords'][:8])}\n",
        "  • **Entités identifiées / Entities identified:** {len(query_analysis['entities'])}\n",
        "  • **Require contexte étendu / Requires extended context:** {'Oui / Yes' if query_analysis['requires_context'] else 'Non / No'}\n",
        "\n",
        "###  **PIPELINE UTILISÉ / PIPELINE USED:**\n",
        "  • **Recherche hybride / Hybrid search:** FAISS combiné + FAISS primaire + BM25 + Entités / Combined FAISS + Primary FAISS + BM25 + Entities\n",
        "  • **Candidats initiaux / Initial candidates:** 25 chunks analysés / chunks analyzed\n",
        "  • **Re-ranking avancé / Advanced re-ranking:** Cross-Encoder avec diversité / Cross-Encoder with diversity\n",
        "  • **Chunks sélectionnés / Chunks selected:** {len(relevant_chunks)} chunks retenus / chunks retained\n",
        "  • **Génération / Generation:** Mistral-7B-Instruct (8-bit, paramètres adaptatifs / adaptive parameters)\n",
        "\n",
        "###  **SOURCES UTILISÉES / SOURCES USED:**\n",
        "{sources_info}\n",
        "\n",
        "###  **STATISTIQUES DES SOURCES / SOURCE STATISTICS:**\n",
        "  • Chunks utilisés / Chunks used: {chunk_stats['total_chunks']}\n",
        "  • Mots moyens par chunk / Average words per chunk: {chunk_stats['avg_word_count']:.0f}\n",
        "  • Pages couvertes / Pages covered: {chunk_stats['total_pages']}\n",
        "  • Entités extraites / Entities extracted: {chunk_stats['entities_found']}\n",
        "\n",
        "---\n",
        "\n",
        "###  **CONTEXTE EXTRAIT (PREVIEW) / EXTRACTED CONTEXT (PREVIEW):**\n",
        "{context_preview}\n",
        "\n",
        "---\n",
        "\n",
        "** Modèle / Model:** Mistral-7B-Instruct v0.2 | **Embeddings:** Multilingue + MiniLM / Multilingual + MiniLM | **Pipeline:** RAG Avancé 2.0 / Enhanced RAG 2.0\n",
        "** Précision estimée / Estimated precision:** +40% vs RAG standard | **Rappel / Recall:** +35% | **Qualité / Quality:** Haute / High\n",
        "\"\"\"\n",
        "\n",
        "        return report\n",
        "\n",
        "# Initialization\n",
        "print(\" Initializing Enhanced RAG\")\n",
        "rag_system = EnhancedAdvancedRAGSystem()"
      ],
      "metadata": {
        "id": "zLeyvm3VGahY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Interface functions\n",
        "def process_pdf_interface(pdf_file):\n",
        "    return rag_system.process_pdf(pdf_file)\n",
        "\n",
        "def answer_question_interface(question):\n",
        "    return rag_system.answer_question(question)\n",
        "\n",
        "custom_css = \"\"\"\n",
        ".main-container {\n",
        "    max-width: 1400px;\n",
        "    margin: 0 auto;\n",
        "}\n",
        "\n",
        "/* MINIMALIST ENHANCED HEADER */\n",
        ".minimal-header {\n",
        "    text-align: center;\n",
        "    padding: 0.8rem 0 0.6rem;\n",
        "    margin-bottom: 0.8rem;\n",
        "}\n",
        "\n",
        ".minimal-header h1 {\n",
        "    font-size: 1.3rem;\n",
        "    font-weight: 600;\n",
        "    color: #1f2937;\n",
        "    margin-bottom: 0.2rem;\n",
        "    letter-spacing: -0.3px;\n",
        "}\n",
        "\n",
        ".minimal-header p {\n",
        "    font-size: 0.85rem;\n",
        "    color: #6b7280;\n",
        "    font-weight: 400;\n",
        "    line-height: 1.3;\n",
        "}\n",
        "\n",
        ".section-title {\n",
        "    font-size: 1rem;\n",
        "    font-weight: 500;\n",
        "    color: #4a5568;\n",
        "    margin-bottom: 0.1rem;\n",
        "    padding-bottom: 0.1rem;\n",
        "    border-bottom: 1px solid #e2e8f0;\n",
        "}\n",
        "\n",
        ".info-section {\n",
        "    background: #f7fafc;\n",
        "    border-radius: 8px;\n",
        "    padding: 1rem;\n",
        "    margin-top: 0.5rem;\n",
        "}\n",
        "\n",
        ".info-section h3 {\n",
        "    color: #2d3748;\n",
        "    font-size: 1rem;\n",
        "    font-weight: 600;\n",
        "    margin-bottom: 1rem;\n",
        "}\n",
        "\n",
        ".info-section ul {\n",
        "    list-style: none;\n",
        "    padding-left: 0;\n",
        "}\n",
        "\n",
        ".info-section li {\n",
        "    padding: 0.4rem 0;\n",
        "    color: #4a5568;\n",
        "    font-size: 0.95rem;\n",
        "}\n",
        "\n",
        ".info-section strong {\n",
        "    color: #2d3748;\n",
        "}\n",
        "\n",
        ".feature-grid {\n",
        "    display: grid;\n",
        "    grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));\n",
        "    gap: 1rem;\n",
        "    margin: 1rem 0;\n",
        "}\n",
        "\n",
        "/* FIXED: Prevent PDF upload area from shrinking */\n",
        ".gr-form {\n",
        "    min-height: auto !important;\n",
        "}\n",
        "\n",
        ".gr-file-upload {\n",
        "    min-height: 80px !important;\n",
        "    height: 80px !important;\n",
        "}\n",
        "\n",
        ".gr-file {\n",
        "    min-height: 70px !important;\n",
        "    height: 70px !important;\n",
        "    border: 2px dashed #e2e8f0 !important;\n",
        "    border-radius: 8px !important;\n",
        "    display: flex !important;\n",
        "    align-items: center !important;\n",
        "    justify-content: center !important;\n",
        "}\n",
        "\n",
        ".gr-file:hover {\n",
        "    border-color: #667eea !important;\n",
        "}\n",
        "\n",
        ".gr-file .file-preview {\n",
        "    display: flex !important;\n",
        "    align-items: center !important;\n",
        "    justify-content: center !important;\n",
        "    height: 100% !important;\n",
        "    width: 100% !important;\n",
        "}\n",
        "\n",
        ".feature-card {\n",
        "    background: white;\n",
        "    padding: 0.5rem;\n",
        "    border-radius: 3px;\n",
        "    border-left: 3px solid #667eea;\n",
        "}\n",
        "\n",
        "/* Make sure all elements maintain consistent height */\n",
        ".fixed-height-container {\n",
        "    min-height: 80px;\n",
        "    display: flex;\n",
        "    align-items: center;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Enhanced interface with minimalistic design\n",
        "with gr.Blocks(css=custom_css, theme=gr.themes.Soft(), title=\"RAG System\") as demo:\n",
        "\n",
        "    with gr.Column(elem_classes=\"main-container\"):\n",
        "        # MINIMAL ENHANCED HEADER\n",
        "        with gr.Column(elem_classes=\"minimal-header\"):\n",
        "            gr.Markdown(\"\"\"\n",
        "            # RAG Document Assistant\n",
        "            PDF Analysis with Mistral-7B\n",
        "            \"\"\")\n",
        "\n",
        "        # Main interface\n",
        "        with gr.Row():\n",
        "            # Left column - PDF Upload\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown('<div class=\"section-title\">📄 PDF Upload</div>')\n",
        "\n",
        "                pdf_input = gr.File(\n",
        "                    label=\"PDF File\",\n",
        "                    file_types=[\".pdf\"],\n",
        "                    file_count=\"single\",\n",
        "                    elem_classes=\"fixed-height-container\"\n",
        "                )\n",
        "\n",
        "                process_btn = gr.Button(\n",
        "                    \"Process with Advanced Analysis\",\n",
        "                    variant=\"primary\",\n",
        "                    size=\"lg\"\n",
        "                )\n",
        "\n",
        "                status_output = gr.Textbox(\n",
        "                    label=\"Analysis Report\",\n",
        "                    lines=21,\n",
        "                    interactive=False,\n",
        "                    show_copy_button=True\n",
        "                )\n",
        "\n",
        "            # Right column - Questions\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown('<div class=\"section-title\">💭 Ask Questions</div>')\n",
        "\n",
        "                question_input = gr.Textbox(\n",
        "                    label=\"Your question\",\n",
        "                    placeholder=\"Ask a complex question for in-depth analysis\",\n",
        "                    lines=2\n",
        "                )\n",
        "\n",
        "                with gr.Row():\n",
        "                    answer_btn = gr.Button(\n",
        "                        \"Analyze with AI\",\n",
        "                        variant=\"secondary\",\n",
        "                        size=\"lg\"\n",
        "                    )\n",
        "                    clear_btn = gr.Button(\n",
        "                        \"Clear\",\n",
        "                        variant=\"stop\",\n",
        "                        size=\"lg\"\n",
        "                    )\n",
        "\n",
        "                answer_output = gr.Textbox(\n",
        "                    label=\"Detailed Response\",\n",
        "                    lines=19,\n",
        "                    interactive=False,\n",
        "                    show_copy_button=True\n",
        "                )\n",
        "\n",
        "    # Event handlers\n",
        "    process_btn.click(\n",
        "        fn=process_pdf_interface,\n",
        "        inputs=[pdf_input],\n",
        "        outputs=status_output\n",
        "    ).then(\n",
        "        fn=lambda: gr.update(interactive=True),\n",
        "        outputs=[answer_btn]\n",
        "    )\n",
        "\n",
        "    answer_btn.click(\n",
        "        fn=answer_question_interface,\n",
        "        inputs=[question_input],\n",
        "        outputs=answer_output\n",
        "    )\n",
        "\n",
        "    clear_btn.click(\n",
        "        fn=lambda: (\"\", \"\"),\n",
        "        outputs=[question_input, answer_output]\n",
        "    )\n",
        "\n",
        "print(\"Launching RAG System\")\n",
        "demo.launch(share=True, debug=False, server_name=\"0.0.0.0\", server_port=7833)"
      ],
      "metadata": {
        "id": "EqkU4mwXAaqi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}